{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from util import get_num_lines, get_pos2idx_idx2pos, index_sequence, get_vocab, embed_indexed_sequence, \\\n",
    "    get_word2idx_idx2word, get_embedding_matrix, write_predictions, get_performance_VUAverb_val\n",
    "from util import TextDatasetWithGloveElmoSuffix as TextDataset\n",
    "from util import evaluate\n",
    "from model import RNNSequenceModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:\n",
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\")\n",
    "print(torch.__version__)\n",
    "#print(\"GPU Detected:\")\n",
    "#print(torch.cuda.is_available())\n",
    "using_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем датасет как list, каждый элемент содержит 3:\n",
    "   * предложение: строка\n",
    "   * индекс рассматриваемого слова\n",
    "   * лэйбл: 1 или 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета:  2436\n",
      "Примеров с лэйблом 1: 48.275862068965516%\n"
     ]
    }
   ],
   "source": [
    "# для датасета \"атмосфера\"\n",
    "raw_mohx = []\n",
    "new_ones, new_zeros = 0, 0\n",
    "ones = 0 # количество лэйблов 1\n",
    "zeros = 0 # количество лэйблов 0\n",
    "\n",
    "with open('../data/атмосфера.csv', encoding='CP1251') as f:\n",
    "    lines = csv.reader(f)\n",
    "    next(lines)\n",
    "    for line in lines:\n",
    "        sentence = line[3]\n",
    "        label_seq = [0] * len(sentence.split())\n",
    "        pos_seq = [0] * len(label_seq)\n",
    "        verb_idx = int(line[4])\n",
    "        verb_label = int(line[5])\n",
    "        if verb_label == 1:\n",
    "            ones += 1\n",
    "            label_seq[verb_idx] = verb_label\n",
    "            pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
    "            raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "            #raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "        else:\n",
    "            zeros += 1\n",
    "            if zeros < 200000:\n",
    "                label_seq[verb_idx] = verb_label\n",
    "                new_zeros += 1\n",
    "                pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
    "                raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "                #raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "\n",
    "print('Размер датасета: ', len(raw_mohx))\n",
    "print('Примеров с лэйблом 1: {}%'.format(ones/(ones+new_zeros)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета:  10000\n",
      "Примеров с лэйблом 1: 47.24%\n"
     ]
    }
   ],
   "source": [
    "# для собранных со всех датасетов, полученных из института русского языка\n",
    "# соотношение классов сделано примерно 50:50\n",
    "raw_mohx = []\n",
    "new_ones, new_zeros = 0, 0\n",
    "ones = 0 # количество лэйблов 1\n",
    "zeros = 0 # количество лэйблов 0\n",
    "\n",
    "datasets = ['атмосфера','ад','агония','ангел','алтарь','архитектор','акула','атом','артерия','алмаз']\n",
    "\n",
    "#with open('../data/MOH-X/MOH-X_formatted_svo_cleaned.csv') as f:\n",
    "#with open('../data/master_marga.csv') as f:\n",
    "for name in datasets:\n",
    "    with open('../data/' + name + '.csv', encoding='CP1251') as f:\n",
    "    # arg1  \targ2\tverb\tsentence\tverb_idx\tlabel\n",
    "        lines = csv.reader(f)\n",
    "        next(lines)\n",
    "        for line in lines:\n",
    "            sentence = line[3]\n",
    "            label_seq = [0] * len(sentence.split())\n",
    "            pos_seq = [0] * len(label_seq)\n",
    "            verb_idx = int(line[4])\n",
    "            verb_label = int(line[5])\n",
    "            if verb_label == 1:\n",
    "                ones += 1\n",
    "                label_seq[verb_idx] = verb_label\n",
    "                pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
    "                raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "                #raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "            else:\n",
    "                zeros += 1\n",
    "                if zeros < 5277:\n",
    "                    label_seq[verb_idx] = verb_label\n",
    "                    new_zeros += 1\n",
    "                    pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
    "                    raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "                    #raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
    "\n",
    "print('Размер датасета: ', len(raw_mohx))\n",
    "print('Примеров с лэйблом 1: {}%'.format(ones/(ones+new_zeros)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеров с лэйблом 1 суммарно без обрезания датасета: 23.129651390520955%\n"
     ]
    }
   ],
   "source": [
    "print('Примеров с лэйблом 1 суммарно без обрезания датасета: {}%'.format(ones/(ones+zeros)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем словарь и эмбеддинги (использовался glove для английского и https://rusvectores.org/ru/about/ для русского)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  96613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189194/189194 [00:10<00:00, 18071.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-trained word vectors loaded:  16690\n",
      "Embeddings mean:  0.0073782335966825485\n",
      "Embeddings stdev:  0.7526317834854126\n"
     ]
    }
   ],
   "source": [
    "# vocab is a set of words\n",
    "vocab = get_vocab(raw_mohx)\n",
    "# two dictionaries. <PAD>: 0, <UNK>: 1\n",
    "word2idx, idx2word = get_word2idx_idx2word(vocab)\n",
    "# glove_embeddings a nn.Embeddings\n",
    "glove_embeddings = get_embedding_matrix(word2idx, idx2word, normalization=False)\n",
    "# elmo_embeddings\n",
    "# set elmos_mohx=None to exclude elmo vectors. Also need to change the embedding_dim in later model initialization\n",
    "#elmos_mohx = h5py.File('../elmo/MOH-X_cleaned.hdf5', 'r')\n",
    "elmos_mohx=None\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(raw_mohx)\n",
    "\n",
    "# second argument is the post sequence, which we don't need\n",
    "embedded_mohx = [[embed_indexed_sequence(example[0], example[2], word2idx,\n",
    "                                      glove_embeddings, elmos_mohx, None),\n",
    "                       example[2], example[1]]\n",
    "                      for example in raw_mohx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-фолдная кросс-валидация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the embedded_sentences and labels into 2 list, in order to pass into the TextDataset as argument\n",
    "sentences = [example[0] for example in embedded_mohx]\n",
    "poss = [example[1] for example in embedded_mohx]\n",
    "labels = [example[2] for example in embedded_mohx]\n",
    "# ten_folds is a list of 10 tuples, each tuple is (list_of_embedded_sentences, list_of_corresponding_labels)\n",
    "ten_folds = []\n",
    "fold_size = int(10000 / 10)\n",
    "for i in range(10):\n",
    "    ten_folds.append((sentences[i * fold_size:(i + 1) * fold_size],\n",
    "                      poss[i * fold_size:(i + 1) * fold_size],\n",
    "                      labels[i * fold_size:(i + 1) * fold_size]))\n",
    "\n",
    "idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
    "\n",
    "optimal_f1s = []\n",
    "optimal_ps = []\n",
    "optimal_rs = []\n",
    "optimal_accs = []\n",
    "predictions_all = []\n",
    "for i in range(10):\n",
    "    '''\n",
    "    2. 3\n",
    "    set up Dataloader for batching\n",
    "    '''\n",
    "    training_sentences = []\n",
    "    training_labels = []\n",
    "    training_poss = []\n",
    "    for j in range(10):\n",
    "        if j != i:\n",
    "            training_sentences.extend(ten_folds[j][0])\n",
    "            training_poss.extend(ten_folds[j][1])\n",
    "            training_labels.extend(ten_folds[j][2])\n",
    "    training_dataset_mohx = TextDataset(training_sentences, training_poss, training_labels)\n",
    "    val_dataset_mohx = TextDataset(ten_folds[i][0], ten_folds[i][1], ten_folds[i][2])\n",
    "    #print(len(val_dataset_mohx))\n",
    "    # Data-related hyperparameters\n",
    "    batch_size = 10\n",
    "    # Set up a DataLoader for the training, validation, and test dataset\n",
    "    train_dataloader_mohx = DataLoader(dataset=training_dataset_mohx, batch_size=batch_size, shuffle=True,\n",
    "                                       collate_fn=TextDataset.collate_fn)\n",
    "    val_dataloader_mohx = DataLoader(dataset=val_dataset_mohx, batch_size=batch_size, shuffle=False,\n",
    "                                     collate_fn=TextDataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка модели, loss и оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Instantiate the model\n",
    "    # embedding_dim = glove + elmo + suffix indicator\n",
    "    # dropout1: dropout on input to RNN\n",
    "    # dropout2: dropout in RNN; would be used if num_layers=1\n",
    "    # dropout3: dropout on hidden state of RNN to linear layer\n",
    "    \n",
    "    # with elmo embedding_dim=300+1024\n",
    "RNNseq_model = RNNSequenceModel(num_classes=2, embedding_dim=300, hidden_size=300, \n",
    "                                    num_layers=1, bidir=True,\n",
    "                                    dropout1=0.5, dropout2=0, dropout3=0)\n",
    "    # Move the model to the GPU if available\n",
    "if using_GPU:\n",
    "    RNNseq_model = RNNseq_model.cuda()\n",
    "    # Set up criterion for calculating loss\n",
    "loss_criterion = nn.NLLLoss()\n",
    "    # Set up an optimizer for updating the parameters of the rnn_clf\n",
    "rnn_optimizer = optim.Adam(RNNseq_model.parameters(), lr=0.001)\n",
    "    # Number of epochs (passes through the dataset) to train the model for.\n",
    "num_epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
      "PRFA performance for  focus verb nan 0.0 nan 53.3\n",
      "Iteration 50. Validation Loss 0.3420531749725342.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99828578005578\n",
      "PRFA performance for  focus verb 40.0 0.04698144233027954 0.09385265133740028 52.68888888888889\n",
      "Iteration 50. Training Loss 0.3445946276187897.\n",
      "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
      "PRFA performance for  focus verb nan 0.0 nan 53.3\n",
      "Iteration 100. Validation Loss 0.3192408084869385.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99951022287308\n",
      "PRFA performance for  focus verb nan 0.0 nan 52.7\n",
      "Iteration 100. Training Loss 0.3218229115009308.\n",
      "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
      "PRFA performance for  focus verb nan 0.0 nan 53.3\n",
      "Iteration 150. Validation Loss 0.3001689016819.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99951022287308\n",
      "PRFA performance for  focus verb nan 0.0 nan 52.7\n",
      "Iteration 150. Training Loss 0.30229049921035767.\n",
      "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
      "PRFA performance for  focus verb nan 0.0 nan 53.3\n",
      "Iteration 200. Validation Loss 0.2829720973968506.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99975511143654\n",
      "PRFA performance for  focus verb 100.0 0.02349072116513977 0.04697040864255518 52.71111111111111\n",
      "Iteration 200. Training Loss 0.28468093276023865.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99777143875913\n",
      "PRFA performance for  focus verb 100.0 0.21413276231263384 0.4273504273504274 53.4\n",
      "Iteration 250. Validation Loss 0.2643125653266907.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99951022287308\n",
      "PRFA performance for  focus verb 100.0 0.04698144233027954 0.0939187602723644 52.72222222222222\n",
      "Iteration 250. Training Loss 0.2667396664619446.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.98662863255483\n",
      "PRFA performance for  focus verb 50.0 0.21413276231263384 0.4264392324093817 53.3\n",
      "Iteration 300. Validation Loss 0.24764877557754517.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99387778591353\n",
      "PRFA performance for  focus verb 100.0 0.18792576932111815 0.37514654161781946 52.78888888888889\n",
      "Iteration 300. Training Loss 0.24821732938289642.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.9754858263505\n",
      "PRFA performance for  focus verb 66.66666666666667 0.4282655246252677 0.8510638297872339 53.4\n",
      "Iteration 350. Validation Loss 0.23294591903686523.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99191867740585\n",
      "PRFA performance for  focus verb 92.3076923076923 0.28188865398167723 0.5620608899297423 52.82222222222222\n",
      "Iteration 350. Training Loss 0.23347671329975128.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.97994294883223\n",
      "PRFA performance for  focus verb 66.66666666666667 0.4282655246252677 0.8510638297872339 53.4\n",
      "Iteration 400. Validation Loss 0.2189546823501587.\n",
      "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.99069423458855\n",
      "PRFA performance for  focus verb 100.0 0.25839793281653745 0.5154639175257731 52.82222222222222\n",
      "Iteration 400. Training Loss 0.21986903250217438.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f446d2dcbf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnum_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iad/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iad/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "performance_matrix = None\n",
    "val_f1 = []\n",
    "val_p = []\n",
    "val_r = []\n",
    "val_acc = []\n",
    "train_f1 = []\n",
    "    # A counter for the number of gradient updates\n",
    "num_iter = 0\n",
    "model_index = 0\n",
    "comparable = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Starting epoch {}\".format(epoch + 1))\n",
    "    for (__, example_text, example_lengths, labels) in train_dataloader_mohx:\n",
    "        example_text = Variable(example_text)\n",
    "        example_lengths = Variable(example_lengths)\n",
    "        labels = Variable(labels)\n",
    "        if using_GPU:\n",
    "            example_text = example_text.cuda()\n",
    "            example_lengths = example_lengths.cuda()\n",
    "            labels = labels.cuda()\n",
    "            # predicted shape: (batch_size, seq_len, 2)\n",
    "        predicted = RNNseq_model(example_text, example_lengths)\n",
    "        batch_loss = loss_criterion(predicted.view(-1, 2), labels.view(-1))\n",
    "        rnn_optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        rnn_optimizer.step()\n",
    "        num_iter += 1\n",
    "            # Calculate validation and training set loss and accuracy every 200 gradient updates\n",
    "        if num_iter % 50 == 0:\n",
    "            avg_eval_loss, performance_matrix = evaluate(idx2pos, val_dataloader_mohx, RNNseq_model,\n",
    "                                                             loss_criterion, using_GPU)\n",
    "            val_loss.append(avg_eval_loss)\n",
    "            val_p.append(performance_matrix[1][0])\n",
    "            val_r.append(performance_matrix[1][1])\n",
    "            val_f1.append(performance_matrix[1][2])\n",
    "            val_acc.append(performance_matrix[1][3])\n",
    "            print(\"Iteration {}. Validation Loss {}.\".format(num_iter, avg_eval_loss))\n",
    "\n",
    "            avg_eval_loss, performance_matrix = evaluate(idx2pos, train_dataloader_mohx, RNNseq_model,\n",
    "                                                              loss_criterion, using_GPU)\n",
    "            train_loss.append(avg_eval_loss)\n",
    "            train_f1.append(performance_matrix[1][2])\n",
    "            print(\"Iteration {}. Training Loss {}.\".format(num_iter, avg_eval_loss))\n",
    "print(\"Training done for fold {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График F1 и Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90875cc5471d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plt.figure(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.title('F1 for moh-x dataset on fold ' + str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xx-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xx-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#plt.figure(0)\n",
    "plt.figure(figsize=(20, 10))\n",
    "#plt.title('F1 for moh-x dataset on fold ' + str(i))\n",
    "plt.title('F1', fontsize='xx-large')\n",
    "plt.xlabel('iteration', fontsize='xx-large')\n",
    "plt.ylabel('F1', fontsize='xx-large')\n",
    "plt.plot(val_f1[:20], 'g')\n",
    "plt.plot(train_f1[:20], 'b')\n",
    "plt.legend(['Validation F1', 'Training F1'], loc='best', fontsize='xx-large')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2477526ae9ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plt.figure(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.title('Loss for moh-x dataset on fold ' + str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xx-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xx-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#plt.figure(1)\n",
    "plt.figure(figsize=(20, 10))\n",
    "#plt.title('Loss for moh-x dataset on fold ' + str(i))\n",
    "plt.title('Loss', fontsize='xx-large')\n",
    "plt.xlabel('iteration', fontsize='xx-large')\n",
    "plt.ylabel('Loss', fontsize='xx-large')\n",
    "plt.plot(val_loss, 'g')\n",
    "plt.plot(train_loss, 'b')\n",
    "plt.legend(['Validation loss', 'Training loss'], loc='best', fontsize='xx-large')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие f1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a092a6f6736c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_f1: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimal_f1s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_f1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_f1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_f1s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_f1' is not defined"
     ]
    }
   ],
   "source": [
    "print('val_f1: ', val_f1)\n",
    "idx = 0\n",
    "if math.isnan(max(val_f1)):\n",
    "    optimal_f1s.append(max(val_f1[6:]))\n",
    "    idx = val_f1.index(optimal_f1s[-1])\n",
    "    optimal_ps.append(val_p[idx])\n",
    "    optimal_rs.append(val_r[idx])\n",
    "    optimal_accs.append(val_acc[idx])\n",
    "else:\n",
    "    optimal_f1s.append(max(val_f1))\n",
    "    idx = val_f1.index(optimal_f1s[-1])\n",
    "    optimal_ps.append(val_p[idx])\n",
    "    optimal_rs.append(val_r[idx])\n",
    "    optimal_accs.append(val_acc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9857276f37b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print('F1 on MOH-X by 10-fold = ', optimal_f1s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision  = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_ps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall  = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_rs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1  = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_f1s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy  = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#print('F1 on MOH-X by 10-fold = ', optimal_f1s)\n",
    "print('Precision  = ', np.mean(np.array(optimal_ps)))\n",
    "print('Recall  = ', np.mean(np.array(optimal_rs)))\n",
    "print('F1  = ', np.mean(np.array(optimal_f1s)))\n",
    "print('Accuracy  = ', np.mean(np.array(optimal_accs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
